<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">

<!-- Mirrored from www.aaronsw.com/weblog/whowriteswikipedia_fn2 by HTTrack Website Copier/3.x [XR&CO'2010], Sun, 13 Jan 2013 00:58:39 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf8"><!-- /Added by HTTrack -->
<head>
  <title>Footnote 2 (Aaron Swartz's Raw Thought)</title>
  <link rel="stylesheet" href="style.css" type="text/css" />
</head>
<body>	
<div class="content">

<p>[ &larr; <a href="whowriteswikipedia.html#fn2">Back to the article</a> ]</p>

<p>The details: I downloaded a copy of the <a href="../../external.html?link=http://download.wikimedia.org/enwiki/">enwiki</a>-<a href="../../external.html?link=http://download.wikimedia.org/enwiki/20060717/">20060717</a>-pages-meta-history.xml.bz2 archive, broke it up into pages, iterated over the revisions and recursively applied Python's <code>difflib.SequenceMatcher.find_longest_match</code> to each revision and the latest revision. (I used <code>find_longest_match</code> instead of <code>get_matching_blocks</code> because <code>get_matching_blocks</code> didn't properly handle blocks being reordered.) I only counted the characters which hadn't already been matched by an earlier revision.</p>

<p>[ &larr; <a href="whowriteswikipedia.html#fn2">Back to the article</a> ]</p>
</body>

<!-- Mirrored from www.aaronsw.com/weblog/whowriteswikipedia_fn2 by HTTrack Website Copier/3.x [XR&CO'2010], Sun, 13 Jan 2013 00:58:39 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf8"><!-- /Added by HTTrack -->
</html>
